{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037bdc4-1e50-4aca-afdc-989652611fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    import os\n",
    "    \n",
    "    # Set GitHub token as an environment variable\n",
    "    os.environ['GITHUB_TOKEN'] = \"gh_\"\n",
    "    \n",
    "    # Remove existing directories if they exist\n",
    "    ! rm -rf sae || True\n",
    "    ! rm -rf TinySQL || True\n",
    "    \n",
    "    # Clone repositories with authentication\n",
    "    ! pip install --upgrade pip\n",
    "    ! git clone https://${GITHUB_TOKEN}@github.com/amirabdullah19852020/sae.git\n",
    "    ! cd sae && git checkout old_saes && pip install .\n",
    "    \n",
    "    ! git clone https://${GITHUB_TOKEN}@github.com/withmartian/TinySQL.git\n",
    "    ! cd TinySQL && pip install .\n",
    "    \n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67c5ac-c850-4cc3-92ec-cf69649ec7c4",
   "metadata": {},
   "source": [
    "### You may need to restart notebook after the dependencies installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb2ec6-62f0-4a8b-b1db-b5391adca63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install circuitsvis nnsight plotly matplotlib einops -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245ed89-37e4-4ad1-b00f-5611e93962cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"SAE_DISABLE_TRITON\"] = \"1\"\n",
    "\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, HTML\n",
    "from typing import Callable\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import sae\n",
    "import torch\n",
    "import torch.fx\n",
    "import einops\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from nnsight import NNsight, LanguageModel\n",
    "from sae import Sae\n",
    "from sae.sae_interp import GroupedSaeOutput, SaeOutput, SaeCollector, LoadedSAES\n",
    "from sae.sae_plotting import plot_layer_curves, plot_layer_features\n",
    "\n",
    "from tqdm import tqdm\n",
    "from TinySQL import sql_interp_model_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f710aa-d933-44f8-b7db-89cbfad237ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current process\n",
    "def process_info():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    # Memory usage in MB\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"RSS: {memory_info.rss / (1024 ** 2):.2f} MB\")  # Resident Set Size\n",
    "    print(f\"VMS: {memory_info.vms / (1024 ** 2):.2f} MB\") \n",
    "\n",
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e60eff-e799-41ff-914a-5a50d55e59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "repo_name = \"withmartian/sql_interp_saes\"\n",
    "cache_dir = \"working_directory\"\n",
    "\n",
    "syn=True\n",
    "\n",
    "full_model_name = sql_interp_model_location(model_num=2, cs_num=1, synonym=syn)\n",
    "model_alias = f\"saes_{full_model_name.split('/')[1]}_syn={syn}\"\n",
    "print(model_alias)\n",
    "\n",
    "\n",
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5716e-54c0-4cf9-bc41-66e398aaf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli login --token hf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e6797-63ea-465d-b290-338581fe5b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repo_path = Path(\n",
    "    snapshot_download(repo_name, allow_patterns=f\"{model_alias}/*\", local_dir=cache_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be9bf3-71a6-4467-baca-81a93e140a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    alpaca_prompt = \"### Instruction: {} ### Context: {} ### Response: {}\"\n",
    "    example['prompt'] = alpaca_prompt.format(example['english_prompt'], example['create_statement'], example['sql_statement'])\n",
    "    example['response'] = example['sql_statement']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbcb18-7a7e-4166-8f60-6aa69d3066e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saes = LoadedSAES.load_from_path(model_alias=model_alias, k=32, cache_dir=cache_dir, dataset_mapper=format_example)\n",
    "model = loaded_saes.language_model\n",
    "tokenizer = loaded_saes.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca7df8-db0b-4c5d-84ad-043d6ed051a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "   def extract_sql_parts(text):\n",
    "       select_part = text[text.find(\"SELECT\") + 7:text.find(\"FROM\")].strip()\n",
    "       from_part = text[text.find(\"FROM\") + 5:].strip()\n",
    "       columns = [c.strip() for c in select_part.split(',')]\n",
    "       return columns, from_part\n",
    "   cols1, from1 = extract_sql_parts(text1)\n",
    "   cols2, from2 = extract_sql_parts(text2)\n",
    "    \n",
    "   score = 0\n",
    "   if \"SELECT\" in text1 and \"SELECT\" in text2: score += 0.2\n",
    "   if \"FROM\" in text1 and \"FROM\" in text2: score += 0.2\n",
    "   if from1 == from2: score += 0.2\n",
    "   if cols1[0] == cols2[0]: score += 0.2\n",
    "   if len(cols1) >= 2 and len(cols2) >= 2:\n",
    "       if cols1[1] == cols2[1]: score += 0.2 \n",
    "   return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f917c2a-d11a-4e8e-bef5-0b0e65bf574b",
   "metadata": {},
   "source": [
    "## Ablation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c62415-4a9e-4529-850e-82190952868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ca3d8-2cfe-496f-9a00-3f335b3abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TinySQL as qts\n",
    "\n",
    "model_num = 2                   # 0=GPT2, 1=TinyStories, 2=Qwen, 3=Llama, 4=Granite, 5=SmolLM\n",
    "cs_num = 1\n",
    "# 0=BaseModel, 1=CS1, 2=CS2 or 3=CS3\n",
    "feature_name = qts.ENGTABLENAME   # ENGTABLENAME, ENGFIELDNAME, DEFTABLESTART, DEFTABLENAME, DEFFIELDNAME, DEFFIELDSEPARATOR\n",
    "use_novel_names = False           # If True, we corrupt using words not found in the clean prompt or create sql e.g. \"little\" or \"hammer\"\n",
    "use_synonyms_table = False\n",
    "use_synonyms_field = False\n",
    "batch_size = 100\n",
    "\n",
    "model = qts.load_tinysql_model(model_num, cs_num, synonym=True)\n",
    "model_hf = qts.sql_interp_model_location(model_num, cs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046d7a5-bc30-4b8f-9f14-93c3cb153466",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = qts.CorruptFeatureTestGenerator(model_num, cs_num, model.tokenizer, use_novel_names=use_novel_names, use_synonyms_field=use_synonyms_field, use_synonyms_table=use_synonyms_table)\n",
    "examples = generator.generate_feature_examples(feature_name, batch_size)\n",
    "\n",
    "# Each examples is corrupted at prompt_token_index. A resulting impact is expected at answer_token_index\n",
    "prompts = []\n",
    "ref_answers = []\n",
    "for i, example in enumerate(examples):\n",
    "    clean_tokenizer_index = example.clean_tokenizer_index\n",
    "    corrupt_tokenizer_index = example.corrupt_tokenizer_index\n",
    "    answer_token_index = example.answer_token_index\n",
    "\n",
    "    # Truncate the clean_prompt at answer_token_index\n",
    "    clean_prompt = example.clean_BatchItem.get_alpaca_prompt() + example.clean_BatchItem.sql_statement\n",
    "    clean_tokens = model.tokenizer(clean_prompt)[\"input_ids\"]\n",
    "\n",
    "\n",
    "    prompts.append(clean_prompt.split('Response: ')[0] + 'Response: ')\n",
    "    ref_answers.append(clean_prompt.split('Response: ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904431a-4817-4ec6-8a3a-7da658a2b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnsight_eval_string_for_layer(layer: str, language_model_var_name=\"language_model\"):\n",
    "    \"\"\"\n",
    "    Converts layer paths to correct format for nnsight evaluation in standalone functions\n",
    "    \"\"\"\n",
    "    # Convert dot notation with numbers to bracket notation\n",
    "    subbed_layer = re.sub(r'\\.([0-9]+)(?=\\.|$)', r'[\\1]', layer)\n",
    "    return f\"{language_model_var_name}.{subbed_layer}.output.save()\"\n",
    "\n",
    "def encode_text_to_activations_for_layer(text: str, layer: str, language_model=model):\n",
    "    with torch.no_grad():\n",
    "        with language_model.trace() as tracer:\n",
    "            with tracer.invoke(text) as invoker:\n",
    "                eval_string = nnsight_eval_string_for_layer(layer, language_model_var_name=\"language_model\")\n",
    "                my_output = eval(eval_string, {\"language_model\": language_model})\n",
    "    \n",
    "    if isinstance(my_output, tuple) and len(my_output) > 1:\n",
    "        return my_output[0]\n",
    "    else:\n",
    "        return my_output\n",
    "        \n",
    "def compute_diff_vector(prompts, num_features, layer_name, language_model=model, sae=sae, tokenizer=tokenizer):\n",
    "    \n",
    "    diff_vectors = []\n",
    "    for prompt in tqdm(prompts, desc=\"Processing prompts\"):\n",
    "        \n",
    "        original_activations = encode_text_to_activations_for_layer(prompt, layer_name, language_model)\n",
    "        sae_encoder_output = sae.encode(original_activations)\n",
    "        top_indices = sae_encoder_output.top_indices[0].cpu().numpy()\n",
    "        top_acts = sae_encoder_output.top_acts[0].detach().cpu().numpy()\n",
    "        \n",
    "        tokens = tokenizer.tokenize(prompt)\n",
    "        sae_output = SaeOutput(\n",
    "            sae_name=layer_name,\n",
    "            sae=sae,\n",
    "            text=prompt,\n",
    "            tokens=tokens,\n",
    "            raw_acts=original_activations.cpu().numpy().tolist(),\n",
    "            top_acts=top_acts.tolist(),\n",
    "            top_indices=top_indices.tolist()\n",
    "        )\n",
    "        \n",
    "        # Find features at the last position\n",
    "        last_position = len(sae_output.top_indices) - 1\n",
    "        last_pos_indices = sae_output.top_indices[last_position]\n",
    "        last_pos_acts = sae_output.top_acts[last_position]\n",
    "        \n",
    "        pairs = sorted(zip(last_pos_indices, last_pos_acts), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get features to ablate (all except top num_features)\n",
    "        target_features_to_ablate = []\n",
    "        if len(pairs) > num_features:\n",
    "            target_features_to_ablate = [idx for idx, _ in pairs[num_features:]]\n",
    "        \n",
    "        # Process features\n",
    "        num_tokens = len(sae_output.top_acts)\n",
    "        num_feats = len(sae_output.top_acts[0])\n",
    "        \n",
    "        new_indices = deepcopy(sae_output.top_indices)\n",
    "        new_acts = deepcopy(sae_output.top_acts)\n",
    "        \n",
    "        # Zero out activations for features to ablate\n",
    "        for i, row in enumerate(new_indices):\n",
    "            for target_feature in target_features_to_ablate:\n",
    "                if target_feature in row:\n",
    "                    index = row.index(target_feature)\n",
    "                    new_acts[i][index] = 0\n",
    "        \n",
    "        # Decode using the original and modified activations\n",
    "        old_vector = sae.decode(top_indices=torch.tensor(sae_output.top_indices).cuda(),\n",
    "                                top_acts=torch.tensor(sae_output.top_acts).cuda())\n",
    "        ablated_vector = sae.decode(top_indices=torch.tensor(new_indices).cuda(),\n",
    "                                    top_acts=torch.tensor(new_acts).cuda())\n",
    "        \n",
    "        # Get the difference vector\n",
    "        difference_vector = ablated_vector - old_vector\n",
    "        diff_vectors.append(difference_vector)\n",
    "    \n",
    "    final_diff = torch.stack(diff_vectors).mean(0)\n",
    "    return final_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c365b6-33a4-414a-9780-af1734a7721c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "def find_similarity_ffeatures(prompts, ref_answers, difference_vector):\n",
    "\n",
    "    simils = []\n",
    "    for i, prompt in tqdm(enumerate(prompts)): \n",
    "        \n",
    "        with model.generate(prompt,max_new_tokens=20, pad_token_id=model.tokenizer.eos_token_id, temperature=0.01, \n",
    "                                top_p=0.9, eos_token_id=model.tokenizer.eos_token_id, do_sample=True, early_stopping=True) as tracer:\n",
    "            \n",
    "            attention_output = model.model.layers[layer_idx].self_attn.output[0].save()\n",
    "            \n",
    "            modified_attention = attention_output[0] + difference_vector\n",
    "            \n",
    "\n",
    "            model.model.layers[layer_idx].self_attn.output = (modified_attention,) + model.model.layers[layer_idx].self_attn.output[1:]\n",
    "\n",
    "            out = model.generator.output.save()\n",
    "    \n",
    "        gen_text = model.tokenizer.decode(out[0], skip_special_tokens=True).split('Response: ')[1]\n",
    "        similarity = calculate_similarity(gen_text, ref_answers[i])\n",
    "        simils.append(similarity)\n",
    "    \n",
    "    print('Total Similarity', sum(simils)/len(simils))\n",
    "\n",
    "    return sum(simils)/len(simils)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7876db-74e8-4536-a64a-2d70adcd3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name =  f'model.layers.{layer_idx}.post_attention_layernorm'\n",
    "sae = loaded_saes.layer_to_saes[layer_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c26d57-4a00-4897-8d31-c2eae6778e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_features(prompts, ref_answers, max_features=32, min_similarity=0.7, window_size=3, flatness_threshold=0.02):\n",
    "    similarities = []\n",
    "    \n",
    "    # Test each feature count\n",
    "    for num in range(1, max_features+1):\n",
    "        diff = compute_diff_vector(prompts, num)\n",
    "        sim = find_similarity_ffeatures(prompts, ref_answers, diff)\n",
    "        similarities.append(sim)\n",
    "        \n",
    "        print(f\"{num} features: {sim:.4f} similarity\")\n",
    "        \n",
    "        # Check if we've exceeded minimum similarity\n",
    "        if sim >= min_similarity:\n",
    "            # Check if the curve has flattened (using sliding window)\n",
    "            if len(similarities) >= window_size:\n",
    "                # Calculate improvement over the last few steps\n",
    "                recent_improvements = [similarities[i] - similarities[i-1] for i in range(len(similarities)-window_size+1, len(similarities))]\n",
    "                avg_improvement = sum(recent_improvements) / len(recent_improvements)\n",
    "                \n",
    "                # If improvement is below threshold, we've flattened\n",
    "                if avg_improvement < flatness_threshold:\n",
    "                    print(f\"Stopping at {num} features - similarity {sim:.4f} has flattened (avg improvement: {avg_improvement:.4f})\")\n",
    "                    return num, sim\n",
    "    \n",
    "    # If we reach max_features or don't find a flattening point\n",
    "    print(f\"Reached maximum {max_features} features with similarity {similarities[-1]:.4f}\")\n",
    "    return max_features, similarities[-1]\n",
    "\n",
    "\n",
    "optimal_features, final_sim = find_optimal_features(\n",
    "    prompts, \n",
    "    ref_answers, \n",
    "    max_features=32, \n",
    "    min_similarity=0.7, \n",
    "    window_size=3, \n",
    "    flatness_threshold=0.02\n",
    ")\n",
    "final_diff = compute_diff_vector(prompts, optimal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3c8ac-fcb2-4713-b8a9-d5ae1e75a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the block size based on the model architecture\n",
    "head_dim = 64   # hidden_size / num_heads = 1024 / 16 = 64\n",
    "num_heads = 14  \n",
    "num_features = optimal_features\n",
    "\n",
    "def analyze_feature_to_heads(feature_num):\n",
    "    with torch.no_grad():\n",
    "        decoder_weights = sae.W_dec  \n",
    "        \n",
    "        feature_vector = decoder_weights[feature_num].clone()\n",
    "        \n",
    "        total_size = feature_vector.numel()\n",
    "        \n",
    "        # For attention layers, we expect the size to be hidden_size (1024)\n",
    "        if total_size != head_dim * num_heads:\n",
    "            print(f\"Warning: Vector size {total_size} doesn't match expected size {head_dim * num_heads}\")\n",
    "            if \"attn\" not in layer_name:\n",
    "                print(f\"This may not be an attention layer: {layer_name}\")\n",
    "                return np.zeros(num_heads)\n",
    "        \n",
    "        # Reshape to match attention heads structure\n",
    "        try:\n",
    "            feature_vector = feature_vector.reshape(num_heads, head_dim)\n",
    "        except RuntimeError:\n",
    "            print(f\"Could not reshape vector of size {total_size} to {num_heads}x{head_dim}\")\n",
    "            # If reshaping fails, just compute overall magnitude\n",
    "            return torch.norm(feature_vector.view(1, -1), dim=1).repeat(num_heads).cpu().numpy()\n",
    "    \n",
    "    # Compute magnitude per head\n",
    "    head_magnitudes = torch.norm(feature_vector, dim=-1).cpu().numpy()\n",
    "    \n",
    "    return head_magnitudes\n",
    "\n",
    "# Select heads based on area under the curve\n",
    "def select_important_heads(heads, magnitudes, contribution_threshold=0.8):\n",
    "\n",
    "    heads = np.array(heads)\n",
    "    magnitudes = np.array([float(m) for m in magnitudes])\n",
    "    \n",
    "    # Sort by magnitude (descending)\n",
    "    sorted_indices = np.argsort(magnitudes)[::-1]\n",
    "    sorted_heads = heads[sorted_indices]\n",
    "    sorted_magnitudes = magnitudes[sorted_indices]\n",
    "    \n",
    "    # Calculate difference from each magnitude to the minimum\n",
    "    min_magnitude = np.min(sorted_magnitudes)\n",
    "    differences = sorted_magnitudes - min_magnitude\n",
    "    \n",
    "    # Calculate total area under the difference curve\n",
    "    total_area = np.sum(differences)\n",
    "    \n",
    "    # Calculate individual and cumulative contributions\n",
    "    contributions = differences / total_area if total_area > 0 else np.zeros_like(differences)\n",
    "    cumulative_contributions = np.cumsum(contributions)\n",
    "    \n",
    "    # Find cutoff index based on contribution threshold\n",
    "    cutoff_indices = np.where(cumulative_contributions >= contribution_threshold)[0]\n",
    "    cutoff_index = cutoff_indices[0] if len(cutoff_indices) > 0 else len(sorted_heads) - 1\n",
    "    \n",
    "    # Get selected heads\n",
    "    selected_heads = sorted_heads[:cutoff_index + 1].tolist()\n",
    "    \n",
    "    analysis = {\n",
    "        'all_heads': sorted_heads,\n",
    "        'all_magnitudes': sorted_magnitudes,\n",
    "        'differences': differences,\n",
    "        'contributions': contributions,\n",
    "        'cumulative_contributions': cumulative_contributions,\n",
    "        'cutoff_index': cutoff_index,\n",
    "        'cutoff_value': sorted_magnitudes[cutoff_index] if cutoff_index < len(sorted_magnitudes) else None,\n",
    "        'total_selected_contribution': cumulative_contributions[cutoff_index] if cutoff_index < len(sorted_magnitudes) else 1.0\n",
    "    }\n",
    "    \n",
    "    return selected_heads, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c198150-09fd-4731-aca9-ba03c80d435b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "accumulated_head_scores = {}\n",
    "\n",
    "for prompt in tqdm(prompts, desc=\"Processing prompts\"):\n",
    "    original_activations = encode_text_to_activations_for_layer(prompt, layer_name)\n",
    "    sae_encoder_output = sae.encode(original_activations)\n",
    "    top_indices = sae_encoder_output.top_indices[0].cpu().numpy()\n",
    "    top_acts = sae_encoder_output.top_acts[0].detach().cpu().numpy()\n",
    "    tokens = tokenizer.tokenize(prompt)  \n",
    "    sae_output = SaeOutput(\n",
    "        sae_name=layer_name,\n",
    "        sae=sae,\n",
    "        text=prompt,  \n",
    "        tokens=tokens,\n",
    "        raw_acts=original_activations.cpu().numpy().tolist(),\n",
    "        top_acts=top_acts.tolist(),\n",
    "        top_indices=top_indices.tolist()\n",
    "    ) \n",
    "    \n",
    "    last_position = len(sae_output.top_indices) - 1\n",
    "    last_position_indices = sae_output.top_indices[last_position]\n",
    "    last_position_acts = sae_output.top_acts[last_position]\n",
    "    \n",
    "    pairs = sorted(zip(last_position_indices, last_position_acts), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    features_kept = []\n",
    "    if len(pairs) > num_features:\n",
    "        features_kept = [idx for idx, _ in pairs[:num_features]]   \n",
    "    else:\n",
    "        features_kept = [idx for idx, _ in pairs]\n",
    "    \n",
    "    \n",
    "    # Process each feature and accumulate head scores\n",
    "    for feature in features_kept:\n",
    "        head_magnitudes = analyze_feature_to_heads(feature)\n",
    "        \n",
    "        for head_idx, magnitude in enumerate(head_magnitudes):\n",
    "            if head_idx not in accumulated_head_scores:\n",
    "                accumulated_head_scores[head_idx] = 0\n",
    "            accumulated_head_scores[head_idx] += magnitude\n",
    "\n",
    "# Sort heads by their total scores\n",
    "sorted_head_scores = sorted(accumulated_head_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nHeads sorted by total score across all prompts:\")\n",
    "for head_idx, total_score in sorted_head_scores:\n",
    "    print(f\"Head {head_idx}: Total score {total_score:.4f}\")\n",
    "\n",
    "# Apply area under the curve analysis to select important heads\n",
    "def select_heads_by_auc(head_scores, contribution_threshold=0.8):\n",
    "    # Extract head indices and scores\n",
    "    heads = np.array(list(head_scores.keys()))\n",
    "    scores = np.array(list(head_scores.values()))\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_heads = heads[sorted_indices]\n",
    "    sorted_scores = scores[sorted_indices]\n",
    "    \n",
    "    # Calculate difference from each score to the minimum\n",
    "    min_score = np.min(sorted_scores)\n",
    "    differences = sorted_scores - min_score\n",
    "    \n",
    "    # Calculate total area under the difference curve\n",
    "    total_area = np.sum(differences)\n",
    "    \n",
    "    # Calculate individual and cumulative contributions\n",
    "    contributions = differences / total_area if total_area > 0 else np.zeros_like(differences)\n",
    "    cumulative_contributions = np.cumsum(contributions)\n",
    "    \n",
    "    # Find cutoff index based on contribution threshold\n",
    "    cutoff_indices = np.where(cumulative_contributions >= contribution_threshold)[0]\n",
    "    cutoff_index = cutoff_indices[0] if len(cutoff_indices) > 0 else len(sorted_heads) - 1\n",
    "    \n",
    "    # Make sure cutoff_index is valid\n",
    "    if cutoff_index >= len(sorted_heads):\n",
    "        cutoff_index = len(sorted_heads) - 1\n",
    "    elif cutoff_index < 0:\n",
    "        cutoff_index = 0\n",
    "    \n",
    "    # Get selected heads\n",
    "    selected_heads = sorted_heads[:cutoff_index + 1].tolist()\n",
    "    \n",
    "    return selected_heads, {\n",
    "        'all_heads': sorted_heads,\n",
    "        'all_scores': sorted_scores,\n",
    "        'cutoff_index': cutoff_index,\n",
    "        'total_selected_contribution': cumulative_contributions[cutoff_index] if cutoff_index < len(cumulative_contributions) else 1.0\n",
    "    }\n",
    "\n",
    "# Select important heads based on 80% area under the curve\n",
    "selected_heads, analysis = select_heads_by_auc(accumulated_head_scores, contribution_threshold=0.85)\n",
    "print(f\"\\nSelected heads (contributing to 80% of area under curve): {selected_heads}\")\n",
    "print(f\"Selection covers {analysis['total_selected_contribution']*100:.2f}% of the area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a87c4-e161-4130-b052-00fb96177104",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335a680-99b6-43d7-bc3e-1033c6a9602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "simils = []\n",
    "for i, prompt in tqdm(enumerate(prompts)): \n",
    "    \n",
    "    with model.generate(prompt,max_new_tokens=20, pad_token_id=model.tokenizer.eos_token_id, temperature=0.01, \n",
    "                            top_p=0.9, eos_token_id=model.tokenizer.eos_token_id, do_sample=True, early_stopping=True) as tracer:\n",
    "        \n",
    "        attention_output = model.model.layers[layer_idx].self_attn.output[0].save()\n",
    "        \n",
    "\n",
    "        modified_attention = attention_output[0] + final_diff\n",
    "        \n",
    "        model.model.layers[layer_idx].self_attn.output = (modified_attention,) + model.model.layers[layer_idx].self_attn.output[1:]\n",
    "        \n",
    "        out = model.generator.output.save()\n",
    "\n",
    "    gen_text = model.tokenizer.decode(out[0], skip_special_tokens=True).split('Response: ')[1]\n",
    "    similarity = calculate_similarity(gen_text, ref_answers[i])\n",
    "    simils.append(similarity)\n",
    "\n",
    "print('Total Similarity', sum(simils)/len(simils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093735b2-9385-46a7-aba6-fd6601164aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "simils = []\n",
    "for i, prompt in tqdm(enumerate(prompts)): \n",
    "    \n",
    "    with model.generate(prompt,max_new_tokens=20, pad_token_id=model.tokenizer.eos_token_id, temperature=0.01, \n",
    "                            top_p=0.9, eos_token_id=model.tokenizer.eos_token_id, do_sample=True, early_stopping=True) as tracer:\n",
    "        \n",
    "        out = model.generator.output.save()\n",
    "\n",
    "    gen_text = model.tokenizer.decode(out[0], skip_special_tokens=True).split('Response: ')[1]\n",
    "    similarity = calculate_similarity(gen_text, ref_answers[i])\n",
    "    simils.append(similarity)\n",
    "\n",
    "print('Total Similarity', sum(simils)/len(simils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41a73e-7e48-48da-9762-32730c90261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEADS = 16\n",
    "\n",
    "def compute_head_means(model, prompt_texts):\n",
    "    layer_means = []  \n",
    "    \n",
    "    with model.generate(prompt_texts,max_new_tokens=7, pad_token_id=model.tokenizer.eos_token_id, temperature=0.5, \n",
    "                        top_p=0.9, eos_token_id=model.tokenizer.eos_token_id, do_sample=True, early_stopping=True) as tracer:\n",
    "        for layer_idx in range(24):\n",
    "            attn_output = model.model.layers[layer_idx].self_attn.output[0]\n",
    "            \n",
    "            output_reshaped = einops.rearrange(\n",
    "                attn_output, \n",
    "                'b s (nh dh) -> b s nh dh',\n",
    "                nh=N_HEADS\n",
    "            )\n",
    "            \n",
    "            # Calculate mean across samples\n",
    "            head_means = output_reshaped.mean(dim=0)  # Shape: [s, nh, dh]\n",
    "            layer_means.append(head_means.save())\n",
    "            \n",
    "    return layer_means\n",
    "\n",
    "\n",
    "\n",
    "def zero_heads_with_means(model, prompt_text, target_layers, heads_per_layer, layer_means, mean=True):\n",
    "    with model.generate(prompt_text,max_new_tokens=7, pad_token_id=model.tokenizer.eos_token_id, temperature=0.5, \n",
    "                        top_p=0.9, eos_token_id=model.tokenizer.eos_token_id, do_sample=True, early_stopping=True) as tracer:\n",
    "        hidden_states = []\n",
    "        for layer_id in range(24):\n",
    "            # Get attention output\n",
    "            attn_output = model.model.layers[layer_id].self_attn.output[0]\n",
    "            target_heads = heads_per_layer[layer_id]\n",
    "            \n",
    "            \n",
    "            output_reshaped = einops.rearrange(\n",
    "                attn_output, \n",
    "                'b s (nh dh) -> b s nh dh',\n",
    "                nh=N_HEADS\n",
    "            )\n",
    "            \n",
    "            head_means = layer_means[layer_id]  # Shape: [s, nh, dh]\n",
    "            # output_reshaped = head_means.expand_as(output_reshaped)\n",
    "            if layer_id not in target_layers:\n",
    "                if mean:\n",
    "                    output_reshaped = head_means.expand_as(output_reshaped)\n",
    "                else:\n",
    "                    output_reshaped = torch.zeros_like(head_means.expand_as(output_reshaped))\n",
    "\n",
    "            if layer_id in target_layers:\n",
    "                for head_idx in range(N_HEADS):\n",
    "                    if head_idx not in target_heads:\n",
    "                        if mean:\n",
    "                            output_reshaped[:, :, head_idx, :] = head_means[:, head_idx, :].unsqueeze(0)\n",
    "                        else:\n",
    "                            output_reshaped[:, :, head_idx, :] = torch.zeros_like(head_means[:, head_idx, :].unsqueeze(0))\n",
    "                        \n",
    "\n",
    "            \n",
    "            # Reshape back to original format\n",
    "            modified_attn = einops.rearrange(\n",
    "                output_reshaped,\n",
    "                'b s nh dh -> b s (nh dh)', \n",
    "                nh=N_HEADS\n",
    "            )\n",
    "            \n",
    "            # Update only the attention output\n",
    "            model.model.layers[layer_id].self_attn.output = (modified_attn,) + model.model.layers[layer_id].self_attn.output[1:]\n",
    "            \n",
    "            # Store the full layer output (which includes both attention and MLP)\n",
    "            hidden_states.append(model.model.layers[layer_id].output[0].save())\n",
    "            \n",
    "        out = model.generator.output.save()\n",
    "        \n",
    "    return hidden_states, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1128a-9b69-4919-8aa6-2aca8486840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_means = compute_head_means(model, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e4490-00e4-4c3f-9aed-fae8f646e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = list(range(24))\n",
    "heads_per_layer = {}\n",
    "for i in range(24):\n",
    "    heads_per_layer[i] = list(range(16))\n",
    "\n",
    "heads_per_layer[layer_idx] = [12, 1, 9, 13, 5, 2, 6]\n",
    "\n",
    "\n",
    "results = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "   hidden_states, output = zero_heads_with_means(model, prompt, target_layers, heads_per_layer, layer_means, mean=False)\n",
    "   gen_text = model.tokenizer.decode(output[0], skip_special_tokens=True).split('Response: ')[1]\n",
    "   similarity = calculate_similarity(gen_text, ref_answers[i])\n",
    "   results.append({\n",
    "       'output': gen_text, \n",
    "       'similarity': similarity\n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bec09-abee-47c3-bb50-1e7f552cc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_similarity = sum(r['similarity'] for r in results)\n",
    "avg_similarity = total_similarity / len(results)\n",
    "print(f\"Average similarity: {avg_similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
